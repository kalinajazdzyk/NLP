{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KAIST AI605 Assignment 3: Tagging, Structured Prediction, HMM, CRF\n",
    "\n",
    "## Rubric\n",
    "\n",
    "### Deadline \n",
    "The deadline for this assignment is: Tuesday 25th October 2022 (Week 9) 11:59pm\n",
    "\n",
    "### Submission\n",
    "Please submit your assignment via [KLMS](https://klms.kaist.ac.kr). You must submit both (1) a PDF of your solutions and (2) the Jupyter Notebook file (.ipynb).\n",
    "\n",
    "Use in-line LaTeX for mathematical expressions. \n",
    "\n",
    "### Collaboration\n",
    "This assignment is not a group assignment so make sure your answer and code are your own.\n",
    "\n",
    "### Grading\n",
    "The total number of marks avaiable is 30 points.\n",
    "\n",
    "### Environment\n",
    "The use of a GPU is not required for this notebook. The required environment for this is Python 3.9. Run the following cell to set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install numpy torch nltk scikit-learn tqdm datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.6rc1\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Tagging Datasets (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 1.1** (2 points) In some tagging problems, such as Named Entity Recognition, IOB and IOBES tagging formats are used. Explain the purpose of these tagging formats and describe the difference between IOB and IOBES tagging formats (max 200 words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose: The purpose of tagging is to catch specific values/names for our task. We want to have specific chuncks from whole sentence. \n",
    "\n",
    "Differences: In IOB we are coding inside, outside and begging with IOBES we are additionally coding ending and single word. IOB is not allowing any nesting like nested Named Entities (ex. Warsaw University of Technology)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 1.2** (2 points) A string is tagged with IOB tagging format as listed below. Write code to extract the tuples of the entities and entity types from the tagged string. (e.g. `[[\"Biden\", \"PERSON\"], ...]`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_string = [\"Biden\", \"was\", \"born\", \"and\", \"raised\", \"in\", \"Scranton\", \",\", \"Pennsylvania\", \",\", \"and\",\n",
    "                \"moved\", \"with\", \"his\", \"family\", \"to\", \"Delaware\", \"in\", \"1953\", \"when\", \"he\", \"was\", \"ten\", \"years\", \"old\", \".\",\n",
    "                \"He\", \"studied\", \"at\", \"the\", \"University\", \"of\", \"Delaware\", \"before\", \"earning\", \"his\", \"law\", \"degree\", \"from\",\n",
    "                \"Syracuse\", \"University\",\".\"]\n",
    "\n",
    "tags          = [\"B-PERSON\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-LOCATION\", \"I-LOCATION\", \"I-LOCATION\", \"O\", \"O\",\n",
    "                \"O\", \"O\", \"O\", \"O\", \"O\", \"B-LOCATION\", \"O\", \"B-DATE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \".\",\n",
    "                \"O\", \"O\", \"O\", \"O\", \"B-ORGANIZATION\", \"I-ORGANIZATION\", \"I-ORGANIZATION\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\",\n",
    "                \"B-ORGANIZATION\", \"I-ORGANIZATION\",\"O\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_list = []\n",
    "for i, string in enumerate(input_string):\n",
    "    tag_list.append((string,tags[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Biden', 'B-PERSON'),\n",
       " ('was', 'O'),\n",
       " ('born', 'O'),\n",
       " ('and', 'O'),\n",
       " ('raised', 'O'),\n",
       " ('in', 'O'),\n",
       " ('Scranton', 'B-LOCATION'),\n",
       " (',', 'I-LOCATION'),\n",
       " ('Pennsylvania', 'I-LOCATION'),\n",
       " (',', 'O'),\n",
       " ('and', 'O'),\n",
       " ('moved', 'O'),\n",
       " ('with', 'O'),\n",
       " ('his', 'O'),\n",
       " ('family', 'O'),\n",
       " ('to', 'O'),\n",
       " ('Delaware', 'B-LOCATION'),\n",
       " ('in', 'O'),\n",
       " ('1953', 'B-DATE'),\n",
       " ('when', 'O'),\n",
       " ('he', 'O'),\n",
       " ('was', 'O'),\n",
       " ('ten', 'O'),\n",
       " ('years', 'O'),\n",
       " ('old', 'O'),\n",
       " ('.', '.'),\n",
       " ('He', 'O'),\n",
       " ('studied', 'O'),\n",
       " ('at', 'O'),\n",
       " ('the', 'O'),\n",
       " ('University', 'B-ORGANIZATION'),\n",
       " ('of', 'I-ORGANIZATION'),\n",
       " ('Delaware', 'I-ORGANIZATION'),\n",
       " ('before', 'O'),\n",
       " ('earning', 'O'),\n",
       " ('his', 'O'),\n",
       " ('law', 'O'),\n",
       " ('degree', 'O'),\n",
       " ('from', 'O'),\n",
       " ('Syracuse', 'B-ORGANIZATION'),\n",
       " ('University', 'I-ORGANIZATION'),\n",
       " ('.', 'O')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2:  HMM and Viterbi Decoding (14 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of a mission to Mars, a rover performs ground surveys to study randomly sampled rocks. The mission is divided into four search areas (quadrants: A,B,C,D). The rover can move north-south and west-east between adjacent search areas such as `(A,B)`, `(A,C)`, but **can not** make diagonal moves between search areas such as `(A,D)` due to physical constraints. \n",
    "\n",
    "![nav.png](nav.png)\n",
    "\n",
    "\n",
    "The rover samples a rock from the search quadrants with the following probabilities:\n",
    "\n",
    "\n",
    "| Rock Type | Area A | Area B | Area C | Area D |\n",
    "|---|---|---|---|---|\n",
    "|Meteorite          | 50% |           20% |   10%  | 0%  |           \n",
    "|Sandstone          | 40% |           30% |   60%  | 90% |          \n",
    "|Igneous Rock       | 10% |           50% |   30%  | 10% |            \n",
    "\n",
    "\n",
    "**Problem 2.1** (2 points) Navigation data sent to and received from the rover is corrupted. State how the path taken by the rover between the search quadrants could be recovered using a first order Hidden Markov Model given observations of the surveyed rocks. Describe modelling assumptions and relevant formula. Ignore start and end states for now.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Assumptions:\n",
    " \n",
    " $P(y) = P(y_1)P(y_2|y_1)...P(y_m|y_1,...,y_{m-1})\\approx\\prod_{t=1}^{m}P(y_t|y_{t-1}) ->$ provided by the restriction of rover movements, each new area to which rover is moving depends on previous movement \n",
    " \n",
    " $X ~ constant$ \n",
    " \n",
    " Formulas: \n",
    " \n",
    " \n",
    " $\\prod_{t=1}^{m}P(y_t|y_{t-1})$ - probability of going to area $y_t$ given previous area $y_{t-1}$\n",
    " \n",
    " From A we can go only to B, C with 50% probability ect. -> assumption \n",
    " \n",
    " $P(x_t|y_t)$ - probability of finding each rock type $x_t$ in given area $y_t$\n",
    " \n",
    " Decision on label: \n",
    " \n",
    " $\\hat Y = argmax\\prod_{t=1}^{m}P(y_t|y_{t-1})P(x_t|y_t) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 2.2** (2 points) Inspection of the corrupted navigation plans indicate that 50% of the time, the rover remains in the same quadrant. Otherwise, the rover randomly navigates along the north-south or west-east axis. Movements in the north-south axis occur twice as often as the west-east axis. Use this information to complete the parameter matrices for your Hidden Markov Model. \n",
    "\n",
    "**Provide your answer as two tensors/matrices using Numpy or PyTorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|            | Area A | Area B | Area C | Area D |\n",
    "|---|---|---|---|---|\n",
    "|Area A      |    50% |        16.67% | 33.34%  |     0% |           \n",
    "|Area B      | 16.67% |           50% |     0%  | 33.34% |          \n",
    "|Area C      | 33.34% |            0% |    50%  | 16.67% |   \n",
    "|Area D      |     0% |        33.34% | 16.67%  |    50% |    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 0.1667, 0.3333, 0.0000],\n",
       "        [0.1667, 0.5000, 0.0000, 0.3333],\n",
       "        [0.3333, 0.0000, 0.5000, 0.1667],\n",
       "        [0.0000, 0.3333, 0.1667, 0.5000]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#area by row, column each rock type\n",
    "p_x_y = torch.Tensor([[0.5,0.4,0.1],[0.2,0.3,0.5],[0.1,0.6,0.3],[0,0.9,0.1]])\n",
    "\n",
    "#area column and area by row \n",
    "p_y_yp = torch.Tensor([[0.5,1/6,2/6,0.],[1/6,0.5,0.,1/3],[1/3,0.,0.5,1/6],[0.,1/3,1/6,0.5]])\n",
    "\n",
    "p_y_yp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 2.3** (2 points) Part of the data recovered from the rover indicates that the rover indicates that the rover identified the following sequence of rock types as `Sandstone, Meteorite, Meteorite, Igneous, Sandstone, Igneous, Sandstone`.\n",
    "\n",
    "An researcher suggests two routes the rover could have taken: `A, A, A, B, D, B, D` or `D, C, C, C, A, A, A`\n",
    "\n",
    "Use this information, combined with your answer from 2.2, to predict which path is most likely. What are the probabilities of these two paths? Do you observe any difference when scoring each path using log probabilities?\n",
    "\n",
    "It is acceptable to use either Numpy or PyTorch for your answer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|           | 0 |             1 |        2|      3 |\n",
    "|---|---|---|---|---|\n",
    "|Meteorite       0   | 50% |           20% |   10%  | 0%  |           \n",
    "|Sandstone       1   | 40% |           30% |   60%  | 90% |          \n",
    "|Igneous Rock    2   | 10% |           50% |   30%  | 10% |   \n",
    "\n",
    "|           | 0 |             1 |        2|      3 |\n",
    "|---|---|---|---|---|\n",
    "|0     |    50% |        16.67% | 33.34%  |     0% |           \n",
    "|1     | 16.67% |           50% |     0%  | 33.34% |          \n",
    "|2     | 33.34% |            0% |    50%  | 16.67% |   \n",
    "|3     |     0% |        33.34% | 16.67%  |    50% |   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_path = [0,0,0,1,3,2,3]\n",
    "s_path = [3,2,2,2,0,0,0]\n",
    "rock_types = [1,0,0,2,1,2,1]\n",
    "#Sandstone, Meteorite, Meteorite, Igneous, Sandstone, Igneous, Sandstone\n",
    "\n",
    "def get_prob(path,rock_types):\n",
    "    previous_area = path[0]\n",
    "    result = 1\n",
    "    for i, area in enumerate(path[1:]):\n",
    "        result = result *p_y_yp[area][previous_area]* p_x_y[area][rock_types[i+1]]\n",
    "        previous_area = path[i+1]\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logprob(path,rock_types):\n",
    "    previous_area = path[0]\n",
    "    result = 1\n",
    "    for i, area in enumerate(path[1:]):\n",
    "        result = result *np.log(p_y_yp[area][previous_area])* np.log(p_x_y[area][rock_types[i+1]])\n",
    "        previous_area = path[i+1]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1728e-05)\n",
      "tensor(1.6673e-07)\n"
     ]
    }
   ],
   "source": [
    "print(get_prob(first_path,rock_types)) # more possible \n",
    "print(get_prob(s_path,rock_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0135)\n",
      "tensor(5.6057)\n"
     ]
    }
   ],
   "source": [
    "# with logs \n",
    "print(get_logprob(first_path,rock_types)) \n",
    "print(get_logprob(s_path,rock_types)) # more possible "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the log probability changes the more possible path taken by the rover. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 2.4** (8 points)  Use the observation history (`Sandstone, Meteorite, Meteorite, Igneous, Sandstone, Igneous, Sandstone`), combined with your answer from 2.2, to decode the most likely position history of the rover using the Viterbi algorithm. \n",
    "\n",
    "Also print the dynamic programming table as well as your most likely position history. It is acceptable to use either Numpy or PyTorch for your answer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viterbi algorith in steps (there is an function implementation at the end, working the same, I was not sure if I should show all the steps or the wole process finished):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 0.4000, 0.1000],\n",
       "        [0.2000, 0.3000, 0.5000],\n",
       "        [0.1000, 0.6000, 0.3000],\n",
       "        [0.0000, 0.9000, 0.1000]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assuming we do not need the starting point \n",
    "# begginig form the second probability computation \n",
    "# p_x_y #emission matrix column -> each rock type, rows -> each area \n",
    "p_x_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 0.1667, 0.3333, 0.0000],\n",
       "        [0.1667, 0.5000, 0.0000, 0.3333],\n",
       "        [0.3333, 0.0000, 0.5000, 0.1667],\n",
       "        [0.0000, 0.3333, 0.1667, 0.5000]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_y_yp #transition matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "cod_area = {area_index: area for area_index, area in enumerate(['A','B','C','D'])}\n",
    "\n",
    "cod_roc = {rock_index: rock for rock_index, rock in enumerate(['Sandstone', 'Meteorite', 'Igneous'])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing needed variables \n",
    "num_areas = 4\n",
    "num_obs = 3\n",
    "T = torch.zeros((num_areas,len(rock_types)))\n",
    "T2 = torch.zeros((num_areas,len(rock_types)))\n",
    "xt = []\n",
    "zt = []\n",
    "\n",
    "# first step of viterbi algorithm\n",
    "# assuming every start is eqyally possible \n",
    "start_prob = torch.repeat_interleave(torch.tensor([1/4]), num_areas)\n",
    "T[:,0] = start_prob*p_x_y[:,rock_types[0]] # one rock type by each area\n",
    "\n",
    "# T2[:, 0] = torch.arange(0,num_areas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second stage of viterbi algorithm \n",
    "# for each observation = rock (except the first one) and each state = area \n",
    "# update the matrix with \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "for obs in range(1,len(rock_types)):\n",
    "    for area in range(0,num_areas):\n",
    "#         print(f'Previous area {T[:,obs-1]}')\n",
    "#         print(f'Prob of moving form area {area} to every other area {p_y_yp[:,area]}')\n",
    "#         print(f'Prob of getting the obseravtion form each area {p_x_y[:,rock_types[obs]]}\\n')\n",
    "        \n",
    "        T[area,obs] = torch.max(T[:,obs-1]*p_y_yp[:,area]*p_x_y[:,rock_types[obs]])\n",
    "        T2[area,obs] = torch.argmax(T[:,obs-1]*p_y_yp[:,area]*p_x_y[:,rock_types[obs]])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e-01, 2.5000e-02, 6.2500e-03, 4.1667e-04, 1.2500e-04, 1.8750e-05,\n",
       "         5.6250e-06],\n",
       "        [7.5000e-02, 8.3333e-03, 2.0833e-03, 5.2083e-04, 1.0417e-04, 2.6042e-05,\n",
       "         5.2083e-06],\n",
       "        [1.5000e-01, 1.6667e-02, 4.1667e-03, 6.2500e-04, 1.8750e-04, 2.8125e-05,\n",
       "         8.4375e-06],\n",
       "        [2.2500e-01, 5.0000e-03, 5.5556e-04, 3.4722e-04, 1.5625e-04, 1.7361e-05,\n",
       "         7.8125e-06]])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T #dynamic matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "zt = []\n",
    "xt = []\n",
    "\n",
    "zt.append(torch.argmax(T[:,obs]))\n",
    "xt.append(zt[0])\n",
    "for i in reversed(range(0, 6)):\n",
    "    zt.insert(0,T2[int(zt[0]),i])\n",
    "    xt.insert(0,zt[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'A', 'A', 'C', 'C', 'C', 'C']"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[cod_area[int(i)] for i in xt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viterbi algorithm in one function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Viterbi_alg(num_areas, num_obs, p_x_y, p_y_yp, obs_list):\n",
    "    # initializing needed variables \n",
    "    T = torch.zeros((num_areas,len(obs_list)))\n",
    "    T2 = torch.zeros((num_areas,len(obs_list)))\n",
    "\n",
    "    # first step of viterbi algorithm\n",
    "    # assuming every start is eqally possible \n",
    "    T[:,0] = (1/4)*p_x_y[:,obs_list[0]] # one rock type by each area\n",
    "    \n",
    "    # second step, uploading dynamic matrix T with probabilieties \n",
    "    # and the matrix T2 with with moves got that best probability \n",
    "    for obs in range(1,len(obs_list)):\n",
    "        for area in range(num_areas):\n",
    "            T[area,obs] = torch.max(T[:,obs-1]*p_y_yp[:,area]*p_x_y[:,rock_types[obs]])\n",
    "            T2[area,obs] = torch.argmax(T[:,obs-1]*p_y_yp[:,area]*p_x_y[:,rock_types[obs]])\n",
    "    \n",
    "    # last step reverse anaysis of best path \n",
    "    zt = []\n",
    "    xt = []\n",
    "\n",
    "    zt.append(torch.argmax(T[:,obs]))\n",
    "    xt.append(zt[0])\n",
    "\n",
    "    for i in reversed(range(0, 6)):\n",
    "        zt.insert(0,T2[int(zt[0]),i])\n",
    "        xt.insert(0,zt[0])\n",
    "\n",
    "    result = [cod_area[int(i)] for i in zt]\n",
    "    return result, T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'A', 'A', 'C', 'C', 'C', 'C']"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result, dynamic_matrix = Viterbi_alg(num_areas, num_obs, p_x_y, p_y_yp, rock_types)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pseudocode was taken form Wikipedia page: https://en.wikipedia.org/wiki/Viterbi_algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Structured Prediction with Conditional Random Fields (12 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 3.1** (2 points) Linear chain conditional random fields have been used in many deep-learning tagging models for natural language processing. Discuss the key difference between this type of Conditional Random Fields and Hidden Markov Models. Highlight why conditional random fields can be used when training deep-learning models. Provide formula as appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden Markov Models are one way dependencies while CRF works in both ways. CRF allowed to create any edges between any pair of nodes. \n",
    "\n",
    "HMM is generative model while CRF is discriminative model. Which means that in HMM formulation we are modeling $P(X,Y)$ thanks to Bayes Rule and with CRF we are directly modeling $P(Y|X)$.\n",
    "\n",
    "CFR we use a shortcut to use scores for edges with constraint on direction (edges potential) not to model emissions and transmissions like in HMM. \n",
    "\n",
    "Difference in modulation: \n",
    "\n",
    "HMM: $P(Y|X) = \\prod_{t=1}^{m} P(x_t|y_t)P(y_t|y_{t-1})$\n",
    "\n",
    "CRF: $P(Y|X) = \\frac{1}{Z(X)}\\prod_{t=1}^{m} \\phi_t(X,y_{t-1},y_t)$ extra normalization $Z(X)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remainder of this problem will use data from the MIT Restaurants dataset. First, download the correct package and load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset mit_restaurant (C:/Users/User/.cache/huggingface/datasets/tner___mit_restaurant/mit_restaurant/1.0.0/cbe6493ab0c7e85b35cdf7fcaadc2c548de45f0e2bd74513f0a36a5288b67e92)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c99220699fba4ff8b86fda5990331d3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('tner/mit_restaurant')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the dataset with the following parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'tags'],\n",
      "        num_rows: 6900\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'tags'],\n",
      "        num_rows: 760\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'tags'],\n",
      "        num_rows: 1521\n",
      "    })\n",
      "})\n",
      "{'tokens': ['can', 'you', 'find', 'me', 'the', 'cheapest', 'mexican', 'restaurant', 'nearby'], 'tags': [0, 0, 0, 0, 0, 9, 14, 0, 5]}\n",
      "{'tokens': ['2', 'start', 'restaurants', 'with', 'inside', 'dining'], 'tags': [1, 2, 0, 0, 3, 4]}\n",
      "{'tokens': ['a', 'four', 'star', 'restaurant', 'with', 'a', 'bar'], 'tags': [0, 1, 2, 0, 5, 6, 3]}\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "print(dataset['train'][0])\n",
    "print(dataset['validation'][0])\n",
    "print(dataset['test'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the dataset is pre-tokenized and that there is a list of tags: one tag for each token. The meaning of the tags is given with the following dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<STOP>\"\n",
    "\n",
    "tag_to_ix = {\n",
    "    \"O\": 0,\n",
    "    \"B-Rating\": 1,\n",
    "    \"I-Rating\": 2,\n",
    "    \"B-Amenity\": 3,\n",
    "    \"I-Amenity\": 4,\n",
    "    \"B-Location\": 5,\n",
    "    \"I-Location\": 6,\n",
    "    \"B-Restaurant_Name\": 7,\n",
    "    \"I-Restaurant_Name\": 8,\n",
    "    \"B-Price\": 9,\n",
    "    \"B-Hours\": 10,\n",
    "    \"I-Hours\": 11,\n",
    "    \"B-Dish\": 12,\n",
    "    \"I-Dish\": 13,\n",
    "    \"B-Cuisine\": 14,\n",
    "    \"I-Price\": 15,\n",
    "    \"I-Cuisine\": 16,\n",
    "    START_TAG: 17,\n",
    "    STOP_TAG: 18\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can construct the vocabulary and word_to_index dictionary as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4166 4166\n"
     ]
    }
   ],
   "source": [
    "vocab = set()\n",
    "for split in dataset.values():\n",
    "    for instance in split:\n",
    "        vocab.update(instance['tokens'])\n",
    "word_to_ix = {word: ix for ix, word in enumerate(vocab)}\n",
    "print(len(vocab), len(word_to_ix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 3.2** (8 points) Using the BaseCRF class provided during the Week 7 lab (code given below), construct and train an LSTM-CRF tagger and report the **recall** for each type of tag on the validation set. It is recommended to use the following configuration for your model. It is acceptable to use sklearn's implementation of the recall function.\n",
    "\n",
    "* Embedding size: 20\n",
    "* LSTM hidden size: 10 \n",
    "* Optimizer: Adam, Learning Rate 0.01, Weight Decay 1e-4\n",
    "* Number of epochs: 1\n",
    "\n",
    "**For speed, limit training to the to the first 1000 instances from the training data.** No additional bonus is provided for using all data. Training may take approx 5-10 minutes. If it takes significantly longer, please check your implementation. It is acceptable to use fewer data if training takes too long.\n",
    "\n",
    "Hints: \n",
    "* the recall for special start/stop tokens may be `NaN` values - remove these.  \n",
    "* the recall for some tags may be zero, this is acceptable as we are not using all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch \n",
    "import math\n",
    "\n",
    "class AI605ConditionalRandomField(nn.Module):\n",
    "\n",
    "    def __init__(self, word_to_ix, tag_to_ix,emb_size,lstm_size):\n",
    "        super(AI605ConditionalRandomField, self).__init__()\n",
    "        self.word_to_ix = word_to_ix\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.vocab_size = len(word_to_ix)\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "        self.emb_size = emb_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.word_embeds = nn.Parameter(torch.randn(self.vocab_size, self.tagset_size))\n",
    "        \n",
    "        # Matrix of transition parameters.  Entry i,j is the score of\n",
    "        # transitioning *to* i *from* j.\n",
    "        self.transitions = nn.Parameter(\n",
    "            torch.randn(self.tagset_size, self.tagset_size))\n",
    "\n",
    "        # These two statements enforce the constraint that we never transfer\n",
    "        # to the start tag and we never transfer from the stop tag\n",
    "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
    "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
    "        \n",
    "        # TODO: Add LSTM, Embedding and Classification Layer\n",
    "        self.embedding = torch.nn.Embedding(self.vocab_size,emb_size)\n",
    "        self.lstm = torch.nn.LSTM(emb_size,lstm_size)\n",
    "        self.linear = torch.nn.Linear(lstm_size,self.tagset_size)\n",
    "\n",
    "        ##############\n",
    "        \n",
    "\n",
    "    # Edit this function\n",
    "    def get_features(self, sentence, position):\n",
    "        # TODO: Complete\n",
    "        #\n",
    "        # sentence: list of strings (words)\n",
    "        # position: int\n",
    "        #\n",
    "        # return: a vector (PyTorch Tensor object) (size:  num_tags = len(tag_to_ix))\n",
    "        #\n",
    "        # Step 1: Convert sentence to a tensor using the self.word_to_ix\n",
    "        # Step 2: Encode the sentence tensor using embeddings, LSTM, classification layer\n",
    "        # Step 3: Return the output from the classification layer for token at position \n",
    "        #         defined by the variable `position` \n",
    "        ######################\n",
    "        sentence = torch.LongTensor([torch.tensor([self.word_to_ix[char]], dtype=torch.long) for char in sentence])\n",
    "        \n",
    "        sent_emb = self.embedding(sentence)\n",
    "#         print(sent_emb[0])\n",
    "#         print(sent_emb.shape)\n",
    "        output, (hn, cn) = self.lstm(sent_emb)\n",
    "#         print(output)\n",
    "#         print(output.shape)\n",
    "        sent_targ = self.linear(output[position])\n",
    "#         print(sent_targ)\n",
    "#         print(sent_targ.shape)\n",
    "        return sent_targ# [position] # The pytorch tensor\n",
    "        #######################\n",
    "\n",
    "                \n",
    "    ########\n",
    "    # There is no need to edit the functions below this point. \n",
    "    # But reading this code might help with your implementation \n",
    "    ########\n",
    "    \n",
    "    # PyTorch Neural Network forward algorithm\n",
    "    def forward(self, sentence):  # dont confuse this with _crf_forward_alg above.\n",
    "        \n",
    "        features = []\n",
    "        for position in range(len(sentence)):\n",
    "            feature = self.get_features(sentence, position)\n",
    "            features.append(feature)\n",
    "        features = torch.vstack(features)\n",
    "\n",
    "        # Find the best path, given the features.\n",
    "        score, tag_seq = self._viterbi_decode(features)\n",
    "        return score, torch.stack(tag_seq)\n",
    "    \n",
    "    def _score_sentence(self, sentence, tags):\n",
    "        # Gives the score of a provided tag sequence\n",
    "        score = torch.zeros(1)\n",
    "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])\n",
    "        for position in range(len(sentence)):\n",
    "            feat = self.get_features(sentence, position)\n",
    "            \n",
    "            try:\n",
    "                score = score + \\\n",
    "                    self.transitions[tags[position + 1], tags[position]] + feat[tags[position + 1]]\n",
    "            except Exception as e:\n",
    "                print(\"If you are seeing this error, then it is likely that your output of get_features function is not correct shape\")\n",
    "                raise e\n",
    "                \n",
    "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
    "        return score\n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        backpointers = []\n",
    "\n",
    "        # Initialize the viterbi variables in log space\n",
    "        init_vvars = torch.full((1, self.tagset_size), -10000.)\n",
    "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "\n",
    "        # forward_var at step i holds the viterbi variables for step i-1\n",
    "        forward_var = init_vvars\n",
    "        for feat in feats:\n",
    "            bptrs_t = []  # holds the backpointers for this step\n",
    "            viterbivars_t = []  # holds the viterbi variables for this step\n",
    "\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
    "                # previous step, plus the score of transitioning\n",
    "                # from tag i to next_tag.\n",
    "                # We don't include the emission scores here because the max\n",
    "                # does not depend on them (we add them in below)\n",
    "                next_tag_var = forward_var + self.transitions[next_tag]\n",
    "                best_tag_id = torch.argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "                \n",
    "            # Now add in the emission scores, and assign forward_var to the set\n",
    "            # of viterbi variables we just computed\n",
    "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)\n",
    "\n",
    "        # Transition to STOP_TAG\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        best_tag_id = torch.argmax(terminal_var)\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "        # Follow the back pointers to decode the best path.\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        # Pop off the start tag (we dont want to return that to the caller)\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "\n",
    "    \n",
    "    def _crf_forward_alg(self, sentence):\n",
    "        # Do the forward algorithm to compute the partition function\n",
    "        init_alphas = torch.full((1, self.tagset_size), -10000.)\n",
    "        # START_TAG has all of the score.\n",
    "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
    "\n",
    "        # Wrap in a variable so that we will get automatic backprop\n",
    "        forward_var = init_alphas\n",
    "\n",
    "        # Iterate through the sentence\n",
    "        for position in range(len(sentence)):\n",
    "            alphas_t = []  # The forward tensors at this timestep\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # broadcast the emission score: it is the same regardless of\n",
    "                # the previous tag\n",
    "                emit_score = self.get_features(sentence, position)\n",
    "                emit_score = emit_score.view(\n",
    "                    1, -1).expand(1, self.tagset_size)\n",
    "                # the ith entry of trans_score is the score of transitioning to\n",
    "                # next_tag from i\n",
    "                trans_score = self.transitions[next_tag].view(1, -1)\n",
    "                # The ith entry of next_tag_var is the value for the\n",
    "                # edge (i -> next_tag) before we do log-sum-exp\n",
    "                next_tag_var = forward_var + trans_score + emit_score\n",
    "                # The forward variable for this tag is log-sum-exp of all the\n",
    "                # scores.\n",
    "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
    "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        alpha = log_sum_exp(terminal_var)\n",
    "        return alpha\n",
    "    \n",
    "    def neg_log_likelihood(self, sentence, tags):\n",
    "        forward_score = self._crf_forward_alg(sentence)\n",
    "        gold_score = self._score_sentence(sentence, tags)\n",
    "        return forward_score - gold_score\n",
    "\n",
    "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
    "def log_sum_exp(vec):\n",
    "    max_score = vec[0, torch.argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "999it [08:56,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  1000] loss: -54.922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import recall_score\n",
    "emb_size = 20 \n",
    "lstm_size = 10 \n",
    "\n",
    "model = AI605ConditionalRandomField(word_to_ix, tag_to_ix,emb_size,lstm_size)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay = 1e-4)\n",
    "\n",
    "loss_sum = 0\n",
    "for epoch in range(1):\n",
    "    \n",
    "    for i , data in tqdm(enumerate(dataset['train'])):\n",
    "        tokens = data['tokens']\n",
    "        tags = data['tags']\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "#         score, tags_pred = model(tokens)\n",
    "\n",
    "        tags = torch.LongTensor(tags)\n",
    "    \n",
    "        loss = model.neg_log_likelihood(tokens, tags)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        loss_sum+= loss.item()\n",
    "        \n",
    "        if(i == 999): break\n",
    "    print(f'[{epoch + 1}, {i + 1:5d}] loss: {loss_sum / (i+1):.3f}')\n",
    "\n",
    "#     print(f'Recall: {recall_score(tags,tags_pred, average = \"macro\", zero_division=1)}')\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "760it [00:11, 69.01it/s] \n"
     ]
    }
   ],
   "source": [
    "# recall on validation set \n",
    "real_tags = []\n",
    "pred_tags = []\n",
    "for i , data in tqdm(enumerate(dataset['validation'])):\n",
    "    tokens = data['tokens']\n",
    "    tags = data['tags']\n",
    "    score, tags_pred = model(tokens)\n",
    "    real_tags = [*real_tags, *tags]\n",
    "    pred_tags = [*pred_tags, *tags_pred]\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.88447992, 0.        , 0.        , 0.31849315, 0.31914894,\n",
       "       0.24458874, 0.60147601, 0.09589041, 0.04672897, 0.04      ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.37459283,\n",
       "       0.        , 0.        ])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(real_tags,pred_tags , average = None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0.8844799248649918,\n",
       " 'B-Rating': 0.0,\n",
       " 'I-Rating': 0.0,\n",
       " 'B-Amenity': 0.3184931506849315,\n",
       " 'I-Amenity': 0.3191489361702128,\n",
       " 'B-Location': 0.24458874458874458,\n",
       " 'I-Location': 0.6014760147601476,\n",
       " 'B-Restaurant_Name': 0.0958904109589041,\n",
       " 'I-Restaurant_Name': 0.04672897196261682,\n",
       " 'B-Price': 0.04,\n",
       " 'B-Hours': 0.0,\n",
       " 'I-Hours': 0.0,\n",
       " 'B-Dish': 0.0,\n",
       " 'I-Dish': 0.0,\n",
       " 'B-Cuisine': 0.3745928338762215,\n",
       " 'I-Price': 0.0,\n",
       " 'I-Cuisine': 0.0}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_scores = {}\n",
    "for tag_name, tag in tag_to_ix.items():\n",
    "    if(tag ==17): break\n",
    "    #     recall_scores.append(recall_score(tags,tags_pred, average = \"macro\", zero_division=1))\n",
    "    #indices for each type of tag \n",
    "    real_tags = np.array(real_tags)\n",
    "    pred_tags = np.array(pred_tags)\n",
    "    indices = np.where(real_tags==tag)\n",
    "    recall = recall_score(real_tags[indices],pred_tags[indices] , average = \"micro\")\n",
    "#     recall = recall_score(real_tags[indices],pred_tags[indices] ,labels = [real_tags[indices][0]], average = None, zero_division=1)\n",
    "    \n",
    "    recall_scores[tag_name] = recall\n",
    "    \n",
    "    \n",
    "recall_scores # tag name : recall "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 3.3** (2 points) Create a copy of of your model and add one additional features to your model:\n",
    "\n",
    "1) The number of characters (the length) of the token\n",
    "\n",
    "Train this model and report the new recall scores. How does this new feature change the model behaviour? \n",
    "\n",
    "Hint: the number of input features for your classifier layer is 11 (LSTM hidden size is 10 + this 1 additional feature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch \n",
    "import math\n",
    "\n",
    "class AI605ConditionalRandomField(nn.Module):\n",
    "\n",
    "    def __init__(self, word_to_ix, tag_to_ix,emb_size,lstm_size):\n",
    "        super(AI605ConditionalRandomField, self).__init__()\n",
    "        self.word_to_ix = word_to_ix\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.vocab_size = len(word_to_ix)\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "        self.emb_size = emb_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.word_embeds = nn.Parameter(torch.randn(self.vocab_size, self.tagset_size))\n",
    "        \n",
    "        # Matrix of transition parameters.  Entry i,j is the score of\n",
    "        # transitioning *to* i *from* j.\n",
    "        self.transitions = nn.Parameter(\n",
    "            torch.randn(self.tagset_size, self.tagset_size))\n",
    "\n",
    "        # These two statements enforce the constraint that we never transfer\n",
    "        # to the start tag and we never transfer from the stop tag\n",
    "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
    "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
    "        \n",
    "        # TODO: Add LSTM, Embedding and Classification Layer\n",
    "        self.embedding = torch.nn.Embedding(self.vocab_size,emb_size)\n",
    "        self.lstm = torch.nn.LSTM(emb_size,lstm_size)\n",
    "        self.linear = torch.nn.Linear(lstm_size + 1,self.tagset_size)\n",
    "\n",
    "        ##############\n",
    "        \n",
    "\n",
    "    # Edit this function\n",
    "    def get_features(self, sentence, position):\n",
    "        # TODO: Complete\n",
    "        #\n",
    "        # sentence: list of strings (words)\n",
    "        # position: int\n",
    "        #\n",
    "        # return: a vector (PyTorch Tensor object) (size:  num_tags = len(tag_to_ix))\n",
    "        #\n",
    "        # Step 1: Convert sentence to a tensor using the self.word_to_ix\n",
    "        # Step 2: Encode the sentence tensor using embeddings, LSTM, classification layer\n",
    "        # Step 3: Return the output from the classification layer for token at position \n",
    "        #         defined by the variable `position` \n",
    "        ######################\n",
    "        sentence = torch.LongTensor([torch.tensor([self.word_to_ix[char]], dtype=torch.long) for char in sentence])\n",
    "        \n",
    "        sent_emb = self.embedding(sentence)\n",
    "\n",
    "        output, (hn, cn) = self.lstm(sent_emb)\n",
    "        \n",
    "        # adding the lenght repeated \n",
    "        x = torch.repeat_interleave(torch.tensor([len(sentence)]), output.shape[0])\n",
    "        \n",
    "        output = torch.hstack((output, x.view(-1,1)))\n",
    "        \n",
    "        sent_targ = self.linear(output)\n",
    "        \n",
    "        return sent_targ[position] # The pytorch tensor\n",
    "        #######################\n",
    "\n",
    "                \n",
    "    ########\n",
    "    # There is no need to edit the functions below this point. \n",
    "    # But reading this code might help with your implementation \n",
    "    ########\n",
    "    \n",
    "    # PyTorch Neural Network forward algorithm\n",
    "    def forward(self, sentence):  # dont confuse this with _crf_forward_alg above.\n",
    "        \n",
    "        features = []\n",
    "        for position in range(len(sentence)):\n",
    "            feature = self.get_features(sentence, position)\n",
    "            features.append(feature)\n",
    "        features = torch.vstack(features)\n",
    "\n",
    "        # Find the best path, given the features.\n",
    "        score, tag_seq = self._viterbi_decode(features)\n",
    "        return score, torch.stack(tag_seq)\n",
    "    \n",
    "    def _score_sentence(self, sentence, tags):\n",
    "        # Gives the score of a provided tag sequence\n",
    "        score = torch.zeros(1)\n",
    "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])\n",
    "        for position in range(len(sentence)):\n",
    "            feat = self.get_features(sentence, position)\n",
    "            \n",
    "            try:\n",
    "                score = score + \\\n",
    "                    self.transitions[tags[position + 1], tags[position]] + feat[tags[position + 1]]\n",
    "            except Exception as e:\n",
    "                print(\"If you are seeing this error, then it is likely that your output of get_features function is not correct shape\")\n",
    "                raise e\n",
    "                \n",
    "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
    "        return score\n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        backpointers = []\n",
    "\n",
    "        # Initialize the viterbi variables in log space\n",
    "        init_vvars = torch.full((1, self.tagset_size), -10000.)\n",
    "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "\n",
    "        # forward_var at step i holds the viterbi variables for step i-1\n",
    "        forward_var = init_vvars\n",
    "        for feat in feats:\n",
    "            bptrs_t = []  # holds the backpointers for this step\n",
    "            viterbivars_t = []  # holds the viterbi variables for this step\n",
    "\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
    "                # previous step, plus the score of transitioning\n",
    "                # from tag i to next_tag.\n",
    "                # We don't include the emission scores here because the max\n",
    "                # does not depend on them (we add them in below)\n",
    "                next_tag_var = forward_var + self.transitions[next_tag]\n",
    "                best_tag_id = torch.argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "                \n",
    "            # Now add in the emission scores, and assign forward_var to the set\n",
    "            # of viterbi variables we just computed\n",
    "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)\n",
    "\n",
    "        # Transition to STOP_TAG\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        best_tag_id = torch.argmax(terminal_var)\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "        # Follow the back pointers to decode the best path.\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        # Pop off the start tag (we dont want to return that to the caller)\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "\n",
    "    \n",
    "    def _crf_forward_alg(self, sentence):\n",
    "        # Do the forward algorithm to compute the partition function\n",
    "        init_alphas = torch.full((1, self.tagset_size), -10000.)\n",
    "        # START_TAG has all of the score.\n",
    "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
    "\n",
    "        # Wrap in a variable so that we will get automatic backprop\n",
    "        forward_var = init_alphas\n",
    "\n",
    "        # Iterate through the sentence\n",
    "        for position in range(len(sentence)):\n",
    "            alphas_t = []  # The forward tensors at this timestep\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # broadcast the emission score: it is the same regardless of\n",
    "                # the previous tag\n",
    "                emit_score = self.get_features(sentence, position)\n",
    "                emit_score = emit_score.view(\n",
    "                    1, -1).expand(1, self.tagset_size)\n",
    "                # the ith entry of trans_score is the score of transitioning to\n",
    "                # next_tag from i\n",
    "                trans_score = self.transitions[next_tag].view(1, -1)\n",
    "                # The ith entry of next_tag_var is the value for the\n",
    "                # edge (i -> next_tag) before we do log-sum-exp\n",
    "                next_tag_var = forward_var + trans_score + emit_score\n",
    "                # The forward variable for this tag is log-sum-exp of all the\n",
    "                # scores.\n",
    "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
    "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        alpha = log_sum_exp(terminal_var)\n",
    "        return alpha\n",
    "    \n",
    "    def neg_log_likelihood(self, sentence, tags):\n",
    "        forward_score = self._crf_forward_alg(sentence)\n",
    "        gold_score = self._score_sentence(sentence, tags)\n",
    "        return forward_score - gold_score\n",
    "\n",
    "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
    "def log_sum_exp(vec):\n",
    "    max_score = vec[0, torch.argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "999it [11:02,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  1000] loss: -97.854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import recall_score\n",
    "emb_size = 20 \n",
    "lstm_size = 10 \n",
    "\n",
    "model = AI605ConditionalRandomField(word_to_ix, tag_to_ix,emb_size,lstm_size)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay = 1e-4)\n",
    "\n",
    "loss_sum = 0\n",
    "for epoch in range(1):\n",
    "    \n",
    "    for i , data in tqdm(enumerate(dataset['train'])):\n",
    "        tokens = data['tokens']\n",
    "        tags = data['tags']\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "#         score, tags_pred = model(tokens)\n",
    "\n",
    "        tags = torch.LongTensor(tags)\n",
    "    \n",
    "        loss = model.neg_log_likelihood(tokens, tags)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        loss_sum+= loss.item()\n",
    "        \n",
    "        if(i == 999): break\n",
    "    print(f'[{epoch + 1}, {i + 1:5d}] loss: {loss_sum / (i+1):.3f}')\n",
    "\n",
    "#     print(f'Recall: {recall_score(tags,tags_pred, average = \"macro\", zero_division=1)}')\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "760it [00:19, 39.18it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'O': 0.8678093449166471,\n",
       " 'B-Rating': 0.012048192771084338,\n",
       " 'I-Rating': 0.0,\n",
       " 'B-Amenity': 0.3767123287671233,\n",
       " 'I-Amenity': 0.43465045592705165,\n",
       " 'B-Location': 0.26406926406926406,\n",
       " 'I-Location': 0.3985239852398524,\n",
       " 'B-Restaurant_Name': 0.06164383561643835,\n",
       " 'I-Restaurant_Name': 0.028037383177570093,\n",
       " 'B-Price': 0.2,\n",
       " 'B-Hours': 0.23529411764705882,\n",
       " 'I-Hours': 0.5803108808290155,\n",
       " 'B-Dish': 0.0,\n",
       " 'I-Dish': 0.0,\n",
       " 'B-Cuisine': 0.3778501628664495,\n",
       " 'I-Price': 0.0,\n",
       " 'I-Cuisine': 0.0136986301369863}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recall on validation set \n",
    "real_tags = []\n",
    "pred_tags = []\n",
    "for i , data in tqdm(enumerate(dataset['validation'])):\n",
    "    tokens = data['tokens']\n",
    "    tags = data['tags']\n",
    "    score, tags_pred = model(tokens)\n",
    "    real_tags = [*real_tags, *tags]\n",
    "    pred_tags = [*pred_tags, *tags_pred]\n",
    "\n",
    "recall_scores = {}\n",
    "for tag_name, tag in tag_to_ix.items():\n",
    "    if(tag ==17): break\n",
    "    #     recall_scores.append(recall_score(tags,tags_pred, average = \"macro\", zero_division=1))\n",
    "    #indices for each type of tag \n",
    "    real_tags = np.array(real_tags)\n",
    "    pred_tags = np.array(pred_tags)\n",
    "    indices = np.where(real_tags==tag)\n",
    "    recall = recall_score(real_tags[indices],pred_tags[indices] , average = \"micro\")\n",
    "#     recall = recall_score(real_tags[indices],pred_tags[indices] ,labels = [real_tags[indices][0]], average = None, zero_division=1)\n",
    "    \n",
    "    recall_scores[tag_name] = recall\n",
    "    \n",
    "    \n",
    "recall_scores # tag name : recall \n",
    "    "
   ]
  },
  {
   "attachments": {
    "obraz.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![obraz.png](attachment:obraz.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is performing similar then previous model but it's performence is slightly better than first model considering this task. It's better discriminating between tokens overall. The conclusion is not very well backed since the models were trained not on full data. Apart from 3 tokens I-Location, B-restaurants and I-Restaurant we see the significant increase in recall score. \n",
    "\n",
    "Based on the loss of model we could assume that this model should perform better then the first one. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
