{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KAIST AI605 Assignment 1: Text Classification\n",
    "\n",
    "## Rubric\n",
    "\n",
    "### Deadline \n",
    "The deadline for this assignment is: Friday 21st October 2022 (Week 8) 11:59pm\n",
    "\n",
    "### Submission\n",
    "Please submit your assignment via [KLMS](https://klms.kaist.ac.kr). You must submit both (1) a PDF of your solutions and (2) the Jupyter Notebook file (.ipynb).\n",
    "\n",
    "Use markdown cells for discussion answers and in-line LaTeX for mathematical expressions. \n",
    "\n",
    "### Collaboration\n",
    "This assignment is not a group assignment so make sure your answer and code are your own.\n",
    "\n",
    "### Grading\n",
    "The total number of marks avaiable is 30 points.\n",
    "\n",
    "### Environment\n",
    "The use of a GPU is not required for this notebook. Run the following cell to set up the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch nltk tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was tested with the following versions of python and torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python 3.8.6rc1\n",
      "torch 1.11.0+cpu\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"python\", python_version())\n",
    "print(\"torch\", torch.__version__) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Text Pre-processing (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This question will use a modified version of the data from the unreliable news dataset released by [Rashkin et al., 2017](https://aclanthology.org/D17-1317/). The dataset contains 15,000 news articles labeled as hoax, sattire, fake news etc. \n",
    "\n",
    "**The data must be downloaded from KLMS**\n",
    "\n",
    "Start by loading the dataset with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def load_data(filename):\n",
    "    with open(filename) as f:\n",
    "        for line in csv.reader(f, delimiter=\"\\t\"):\n",
    "            yield line\n",
    "\n",
    "train_data = list(load_data(\"assignment1_data/train.tsv\"))\n",
    "test_data = list(load_data(\"assignment1_data/test.tsv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each item in `train_data` and `test_data` is a (2-tuple) of the label and sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12003\n",
      "2997\n",
      "\n",
      "['Satire', \"GREEN BAY, WIDavid Horsted, 45, announced Monday that he's seen a whole heck of a lot during his 20 years driving a taxi. 'Aw, geez, the people I've met and the places I've seenthe stories would make your head spin,' Horsted said. 'I've been from Lambeau Field to the Barhausen Waterfowl Preserve and every place in between. One time, one of the Packers even threw up in my cab, but I don't think I should say who.' With a little prodding, Horsted said the person's first name rhymes with 'baloney' and last name with 'sandwich.' \"]\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(test_data))\n",
    "print()\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 1.1** (1 point) Use NLTK's `word_tokenize` to tokenize each document in the `train_data` and `test_data` and store these in two lists: `train_tokenized` and `test_tokenized`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Train Output:\n",
      "['Satire', ['GREEN', 'BAY', ',', 'WIDavid', 'Horsted', ',', '45', ',', 'announced', 'Monday', 'that', 'he', \"'s\", 'seen', 'a', 'whole', 'heck', 'of', 'a', 'lot', 'during', 'his', '20', 'years', 'driving', 'a', 'taxi', '.', \"'Aw\", ',', 'geez', ',', 'the', 'people', 'I', \"'ve\", 'met', 'and', 'the', 'places', 'I', \"'ve\", 'seenthe', 'stories', 'would', 'make', 'your', 'head', 'spin', ',', \"'\", 'Horsted', 'said', '.', \"'\", 'I', \"'ve\", 'been', 'from', 'Lambeau', 'Field', 'to', 'the', 'Barhausen', 'Waterfowl', 'Preserve', 'and', 'every', 'place', 'in', 'between', '.', 'One', 'time', ',', 'one', 'of', 'the', 'Packers', 'even', 'threw', 'up', 'in', 'my', 'cab', ',', 'but', 'I', 'do', \"n't\", 'think', 'I', 'should', 'say', 'who', '.', \"'\", 'With', 'a', 'little', 'prodding', ',', 'Horsted', 'said', 'the', 'person', \"'s\", 'first', 'name', 'rhymes', 'with', \"'baloney\", \"'\", 'and', 'last', 'name', 'with', \"'sandwich\", '.', \"'\"]]\n",
      "\n",
      "Example Test Output:\n",
      "['Propaganda', ['The', 'Independents', 'Grill', 'NSA', 'Defender', 'On', 'Legality', 'Of', 'Surveillance', 'ProgramYoutube']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "train_tokenized = []\n",
    "test_tokenized = []\n",
    "for element in train_data:\n",
    "    train_tokenized.append([element[0],word_tokenize(element[1])])\n",
    "\n",
    "\n",
    "print(\"Example Train Output:\")\n",
    "print(train_tokenized[0])\n",
    "print()\n",
    "for element in test_data:\n",
    "    test_tokenized.append([element[0],word_tokenize(element[1])])\n",
    "\n",
    "print(\"Example Test Output:\")\n",
    "print(test_tokenized[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 1.2** (2 points) isntead of using NLTK's tokenizer, show splitting the string by whitespace for a small sample of instances. Discuss two limitations when string splitting with whitespace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Split Output:\n",
      "['Satire', ['GREEN', 'BAY,', 'WIDavid', 'Horsted,', '45,', 'announced', 'Monday', 'that', \"he's\", 'seen', 'a', 'whole', 'heck', 'of', 'a', 'lot', 'during', 'his', '20', 'years', 'driving', 'a', 'taxi.', \"'Aw,\", 'geez,', 'the', 'people', \"I've\", 'met', 'and', 'the', 'places', \"I've\", 'seenthe', 'stories', 'would', 'make', 'your', 'head', \"spin,'\", 'Horsted', 'said.', \"'I've\", 'been', 'from', 'Lambeau', 'Field', 'to', 'the', 'Barhausen', 'Waterfowl', 'Preserve', 'and', 'every', 'place', 'in', 'between.', 'One', 'time,', 'one', 'of', 'the', 'Packers', 'even', 'threw', 'up', 'in', 'my', 'cab,', 'but', 'I', \"don't\", 'think', 'I', 'should', 'say', \"who.'\", 'With', 'a', 'little', 'prodding,', 'Horsted', 'said', 'the', \"person's\", 'first', 'name', 'rhymes', 'with', \"'baloney'\", 'and', 'last', 'name', 'with', \"'sandwich.'\", '']]\n"
     ]
    }
   ],
   "source": [
    "split_train_tokenized = []\n",
    "for element in train_data:\n",
    "    split_train_tokenized.append([element[0],element[1].split(\" \")])\n",
    "print(\"Example Split Output:\")\n",
    "print(split_train_tokenized[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limitation 1:\n",
    "All the commans are attached to the words which make it harder to analyse them. Same goes for every special character in dictionary. Could be potentially problematic in networks, words *chocolate* and *chocolate,* would be treated differently. Of couse we could train first the smaller network to omit special characters and treat every word with such attachment equally.\n",
    "\n",
    "\n",
    "\n",
    "Limitation 2:\n",
    "Our vocabulary would be bigger and a lot of words would be doubled like: *he* and *he's* but also the sentense would be not interpreted correctly. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 1.3** (1 points) construct a vocabulary of all tokens in `train_tokenized`. Add an extra `UNK` token as a placeholder to account for unknown/unseen tokens at test time. How many unique tokens are present in this vocabulary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147722\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "words_counts = Counter()\n",
    "\n",
    "for words in train_tokenized:\n",
    "    words_counts.update(words[1])\n",
    "\n",
    "print(len(words_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {word: i for i, (word,count) in enumerate(words_counts.items())}\n",
    "vocab[\"<UNK>\"] = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147723"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 1.4** (2 points) We can reduce the size of the vocabulary by removing less frequently occuring words. Create a vocabulary for tokens in the which only contains tokens occuring at least (>=) 5 times. What is the size of the vocabulary now? (Remember to include the UNK placeholder token for unseen tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full vocabullary count: 147723, limitted vocabullary count: 39228\n"
     ]
    }
   ],
   "source": [
    "limitted_vocab = {word: count for word, count in words_counts.items() if count>= 5 }\n",
    "limitted_vocab[\"<UNK>\"] = 0\n",
    "print(f'Full vocabullary count: {len(vocab)}, limitted vocabullary count: {len(limitted_vocab)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 1.5** (1 point) suggest how additional pre-processing could be used to reduce the vocabulary size when tokenizing with NLTK's tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "670"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "limitted_vocab['Here']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3638"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "limitted_vocab['here']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"*'s*\", \"*'ve*\", \"*n't*\" - catalloging those shortcuts as proper words ve -> have ect. \n",
    "\n",
    "eliminating 'CISA', 'SARTRE' as one type of wrods maybe 'proper name' \n",
    "and maybe the same goes with names we do not need them specificaly for some tasks\n",
    "so overall with proper names we could group them and get one label and count them together \n",
    "\n",
    "also getting every word with upper and lower letter together would reduce the size significantly but that procedure would depend on what tasks our neural network is suppose to perform "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 1.6** (1 point) Print the number of items in each class for the test dataset (`test_data`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Propaganda\n"
     ]
    }
   ],
   "source": [
    "for words in test_tokenized:\n",
    "    print(words[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only_labels = lambda string: string[0]\n",
    "# only_labels(only_labels(test_tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [label for label,string in test_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Propaganda': 992, 'Hoax': 986, 'Satire': 1019})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_count = Counter(labels)\n",
    "label_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Training a feed-forward network (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 2.1** (1 point) Create a dictionary of label names to a unique index and call this `label2idx`, create a dictionary of unique words from the vocabulary to an index and call this `word2idx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {word: index for index,word in enumerate(vocab)}\n",
    "# word2idx[\"<UNK>\"] = len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147723"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "label2idx = {label: i for i,label in enumerate(np.unique(labels))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Hoax': 0, 'Propaganda': 1, 'Satire': 2}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 2.2** (1 point) For each document in `train_tokenized` and `test_tokenized`, create a `torch.LongTensor` of the word IDs from `word2idx`. If a word does not appear in `word2idx`, replace it with a special token for unknwon values (`UNK`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_in_vocab(ch):\n",
    "    try: \n",
    "        return word2idx[ch]\n",
    "    except KeyError:\n",
    "        return word2idx[\"<UNK>\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectorized = []\n",
    "for label, sent in train_tokenized:\n",
    "    ids = [not_in_vocab(ch) for ch in sent]    \n",
    "    train_vectorized.append((torch.LongTensor(ids), torch.LongTensor([label2idx[label]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([262])\n"
     ]
    }
   ],
   "source": [
    "print(train_vectorized[12][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vectorized = []\n",
    "for label, sent in test_tokenized:\n",
    "    ids = [not_in_vocab(ch) for ch in sent]   \n",
    "    test_vectorized.append((torch.LongTensor(ids), torch.LongTensor([label2idx[label]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Propaganda',\n",
       " ['The',\n",
       "  'Independents',\n",
       "  'Grill',\n",
       "  'NSA',\n",
       "  'Defender',\n",
       "  'On',\n",
       "  'Legality',\n",
       "  'Of',\n",
       "  'Surveillance',\n",
       "  'ProgramYoutube']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tokenized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32691"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx['Independents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   108,  32691,  17439,    289,  31697,   3929, 118474,   3077,  12715,\n",
      "        147722])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "147722"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(test_vectorized[0][0])\n",
    "word2idx['<UNK>']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 2.3** (3 points) Create a Multi Layer Perceptron to classify these documents that performs the following operations: \n",
    "\n",
    "* (1) uses `torch.Embedding` to create a $d$-dimensional continuous representation of each token (`embedding_size`), \n",
    "* (2) sums the word embeddings, \n",
    "* (3) uses `tanh` activation function, \n",
    "* (4) uses a torch.Linear layer to project to a hidden dimension (`hidden_size`), \n",
    "* (5) applies tanh activation to this hidden representation\n",
    "* (6) uses a linear layer to perform classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 20\n",
    "hidden_size = 10\n",
    "classes = label2idx.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(torch.nn.Module):\n",
    "    def __init__(self,vocab,embedding_size, hidden_size, classes):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding((len(vocab)+1),embedding_size) #maybe using EmmbedingBag???\n",
    "        self.linear = torch.nn.Linear(embedding_size, hidden_size)\n",
    "        self.activation = torch.nn.Tanh()\n",
    "        self.classification = torch.nn.Linear(hidden_size, len(classes))\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.embedding(x).sum(dim = 1))\n",
    "        x = self.activation(self.linear(x))\n",
    "        x = self.classification(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 2.4** (3 points) train the model for 3 epochs and report the mean loss and the accuracy on the test set at each epoch. Use cross-entropy loss and the `Adam` optimizer (https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) with the learning rate set to `0.005`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Perceptron = MultiLayerPerceptron(vocab, embedding_size, hidden_size, classes)  \n",
    "cross_entropy = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(Perceptron.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12003it [09:16, 21.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 12003] loss: 0.323\n",
      "0.9102435769102436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12003it [10:05, 19.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 12003] loss: 0.108\n",
      "0.9526192859526192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12003it [09:51, 20.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 12003] loss: 0.061\n",
      "0.9679679679679679\n",
      "complete training\n"
     ]
    }
   ],
   "source": [
    "# loop over the dataset multiple times\n",
    "for epoch in range(3):  \n",
    "\n",
    "    runloss = 0.0\n",
    "    for x, data in tqdm(enumerate(train_vectorized)):\n",
    "        # get the inputs; data is [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = Perceptron(inputs.view(1,-1))\n",
    "        loss = cross_entropy(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        runloss += loss.item()\n",
    "    \n",
    "    \n",
    "    print(f'[{epoch + 1}, {x + 1:5d}] loss: {runloss / 12000:.3f}')\n",
    "    \n",
    "    #testing \n",
    "    acc = 0\n",
    "    for test_tokens, label in test_vectorized:\n",
    "        predictions = Perceptron(test_tokens.view(1,-1))\n",
    "        predicted = torch.argmax(predictions.data,1)\n",
    "\n",
    "        if(predicted==label):\n",
    "            acc += 1\n",
    "    print(acc/len(test_vectorized))\n",
    "print('complete training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Recurrent Neural Networks (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that a recurrent neural network conditionally encodes tokens given the previous hidden state $\\mathbf{h}_{t-1}$ and the input at the current input $\\mathbf{x}_t$:\n",
    "\n",
    "$$\n",
    "\\mathbf{h}_t = \\tanh (U\\mathbf{h}_{t-1} + V\\mathbf{x}_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 3.1** (1 point) Show that such recurrent neural network (RNN) without an activation function is equivalent to a single linear transformation with respect to the inputs, which means each $\\textbf{h}_t$ is a linear combination of the inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 3.2** (4 points) Provide your own implementation of an RNN cell (i.e. do NOT use the built in `torch.nn.RNN` method) that conforms to the following specification:\n",
    "\n",
    "* Input: a matrix of $N$ embeddings of dimension size $d$ describing a sequence of embeddings for tokens (matrix size: $\\mathbb{R}^{N \\times d}$)\n",
    "* Output: a tuple containing \n",
    "  * 1: A matrix containing the $N$ hidden states of dimension size $b$ (matrix size: $\\mathbb{R}^{N \\times b}$) \n",
    "  * 2: the final hidden state of the last element (vector of size $\\mathbb{R}^{b}$)\n",
    "  \n",
    "* Assume that the initial hidden state is a vector of zeros ($\\mathbf{h}_0 = [0,...,0]^T$)\n",
    "* Including bias term is optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLayer(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self,embedding_size , hidden_size):\n",
    "        super(RNNLayer, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.initial_h = torch.zeros(hidden_size)\n",
    "        # weights for netwrok \n",
    "        self.U = torch.FloatTensor(hidden_size, hidden_size).uniform_(0,1)\n",
    "        \n",
    "        self.V = torch.FloatTensor(hidden_size, embedding_size).uniform_(0,1)\n",
    "        \n",
    "    def forward(self, inputs): \n",
    "        hs, ycap = {}, {} #pytorch arrays\n",
    "        \n",
    "        hs[-1] = self.initial_h\n",
    "#         _, N, d = inputs.shape\n",
    "        N, d = inputs.shape\n",
    "        for i in range(N):\n",
    "            hs[i] = torch.tanh(torch.add(torch.matmul(self.U,hs[i-1]),torch.matmul(self.V,inputs[i]))) \n",
    "        return (hs.values(), hs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 3.3** (1 point) create a copy of your model from problem 2.3 and change the summing of embeddings to instead use the final hidden state of your own RNN implementation. Use a copy of your training code from problem 2.4 and modify it to train your model on the first 100 items of the training set, reporting the mean loss and the accuracy on the first 100 items of the test set.\n",
    "\n",
    "*NOTE: Because training is slow, we limit the training and test data to 100 samples. There is no additional award for using all data*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class copy_of_model(torch.nn.Module):\n",
    "    def __init__(self,vocab_size,embedding_size, hidden_size, classes):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_size) \n",
    "        self.rnn_layer = RNNLayer(embedding_size, hidden_size) \n",
    "#         self.linear =  torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.activation = torch.nn.Tanh()\n",
    "        self.classification = torch.nn.Linear(hidden_size, len(classes))\n",
    "         \n",
    "\n",
    "    def forward(self, x):  \n",
    "        x = self.embedding(x)\n",
    "        hidden_sattes, x = self.rnn_layer(x) \n",
    "#         x = self.linear(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.classification(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 1.104\n",
      "0.49\n",
      "[2,   100] loss: 1.007\n",
      "0.44\n",
      "[3,   100] loss: 0.978\n",
      "0.42\n",
      "complete training\n"
     ]
    }
   ],
   "source": [
    "RNN_model = copy_of_model(len(vocab), embedding_size, hidden_size, classes)  \n",
    "cross_entropy = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(RNN_model.parameters(), lr=0.005)\n",
    "# loop over the dataset multiple times\n",
    "for epoch in range(3):  \n",
    "\n",
    "    runloss = 0.0\n",
    "    for x, data in enumerate(train_vectorized):\n",
    "        # get the inputs; data is [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = RNN_model(inputs) #.view(1,-1))\n",
    "        loss = cross_entropy(outputs.view(1,-1), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        runloss += loss.item()\n",
    "        \n",
    "        #only 100 iterations\n",
    "        if x == 99:    \n",
    "            break\n",
    "    \n",
    "    #printing the loss\n",
    "    print(f'[{epoch + 1}, {x + 1:5d}] loss: {runloss/100:.3f}')\n",
    "    \n",
    "    #testing on test_data\n",
    "    acc = 0\n",
    "    for i, data in enumerate(test_vectorized):\n",
    "        test_tokens, label = data\n",
    "        predictions = RNN_model(test_tokens)\n",
    "        predicted = torch.argmax(predictions)\n",
    "        if(predicted==label):\n",
    "            acc += 1\n",
    "        if(i==99):\n",
    "            break\n",
    "    print(acc/100)\n",
    "        \n",
    "print('complete training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 3.4** (2 points) A limitation of the RNN is the vanishing gradient and exploding gradient problem. Exploding gradients can be mitigated with gradient clipping. Describe the method and benefit of gradient clipping and provide a simple implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploiding gradient is a large expotential increase of weights. To prevent our Neural Network we can use method called gradient clipping. This method changes the derivative of the error before backpropagation step by rescalling the gradients with vector norm or clipping gradient values which exceed the given range. This two aproches are called: Gradient clipping by value and Gradient clipping by norm. First aproch specifies the minimum and maximum clip value and if the gradient exceeds this threshold value is clipped with the given minimum and maximum. Second aproch is also clipping the gradinet value but by multiplying the unit vector of gradients with the threshold.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_clipping(weights, upper_bound, lower_bound):\n",
    "    weights[weights>upper_boud] = upper_bound\n",
    "    weights[weights<lower_bound] = lower_bound\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: LSTM (6 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 4.1** (2 points) Explain how the architecture of an LSTM can mitigate the vanishing gradient problem found in RNNs. (A complete proof is not necessary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the cell state derivative that can prevent the LSTM gradients from vanishing,\n",
    "make the series of sub gradientss in (3) not converge to zero,\n",
    "It is the presence of the forget gateâ€™s vector of activations in the gradient term along with additive structure which allows the LSTM to find such a parameter update at any time step\n",
    "Another important property to notice is that the cell state gradient is an additive function\n",
    "In RNNs, the sum in (3) is made from expressions with a similar behaviour that are likely to all be in [0,1] which causes vanishing gradients.\n",
    "\n",
    "In LSTMs, however, the presence of the forget gate, along with the additive property of the cell state gradients, enables the network to update the parameter in such a way that the different sub gradients in (3) do not necessarily agree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 4.2** (4 points) Provide your own implementation of the LSTM (i.e. do NOT use the built in `torch.nn.LSTM` method) that conforms to the following specification:\n",
    "\n",
    "* Input: a matrix of $N$ embeddings of dimension size $d$ describing a sequence of embeddings for tokens (matrix size: $\\mathbb{R}^{N \\times d}$)\n",
    "* Output: a tuple containing \n",
    "  * 1: A matrix containing the $N$ hidden states of dimension size $b$ (matrix size: $\\mathbb{R}^{N \\times b}$) \n",
    "  * 2: the final hidden state of the last element (vector of size $\\mathbb{R}^{b}$)\n",
    "  \n",
    "* Assume that the initial hidden state is a vector of zeros ($\\mathbf{h}_0 = [0,...,0]^T$)\n",
    "* Assume that the initial cell state is a vector of zeros ($\\mathbf{c}_0 = [0,...,0]^T$)\n",
    "* Including bias term is optional\n",
    "\n",
    "\n",
    "Following problem 3.3, demonstrate training on the first 100 instances instances from the training set and report the accuracy and loss on the first 100 instances from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLayer(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self,embedding_size , hidden_size):\n",
    "        super(LSTMLayer, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.initial_h = torch.zeros(hidden_size)\n",
    "        \n",
    "        # weights for x\n",
    "        self.Wxf = torch.FloatTensor(hidden_size, embedding_size).uniform_(0,1)\n",
    "        self.Wxi = torch.FloatTensor(hidden_size, embedding_size).uniform_(0,1)\n",
    "        self.Wxc = torch.FloatTensor(hidden_size, embedding_size).uniform_(0,1)\n",
    "        self.Wxo = torch.FloatTensor(hidden_size, embedding_size).uniform_(0,1)\n",
    "        \n",
    "        # weights for h\n",
    "        self.Whf = torch.FloatTensor(hidden_size, hidden_size).uniform_(0,1)\n",
    "        self.Whi = torch.FloatTensor(hidden_size, hidden_size).uniform_(0,1)\n",
    "        self.Whc = torch.FloatTensor(hidden_size, hidden_size).uniform_(0,1)\n",
    "        self.Who = torch.FloatTensor(hidden_size, hidden_size).uniform_(0,1)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x): \n",
    "        hs, ct, ycap = {},{}, {} #pytorch arrays\n",
    "        \n",
    "        ct[-1] = torch.zeros(hidden_size).reshape(-1)\n",
    "        hs[-1] = self.initial_h\n",
    "        N, d = x.shape\n",
    "        \n",
    "        for i in range(N):\n",
    "\n",
    "            it = torch.sigmoid(torch.add(torch.matmul(self.Wxi, x[i]),torch.matmul(self.Whi,hs[i-1].reshape(-1))))\n",
    "\n",
    "            ft = torch.sigmoid(torch.add(torch.matmul(self.Wxf, x[i]),torch.matmul(self.Whf,hs[i-1].reshape(-1))))\n",
    "\n",
    "#             print(f'FT shape: {ft.shape}')\n",
    "            Ct = torch.tanh(torch.add(torch.matmul(self.Wxc, x[i]),torch.matmul(self.Whc,hs[i-1].reshape(-1))))\n",
    "\n",
    "#             print(f'Ct shape: {Ct.shape}')\n",
    "            \n",
    "            ct[i] = torch.add(torch.mul(ft,ct[i-1]),torch.mul(it,Ct).reshape(-1))\n",
    "#             print(f'ct shape: {ct[i].shape}')\n",
    "            ot = torch.sigmoid(torch.add(torch.matmul(self.Wxo,x[i]),torch.matmul(self.Who,hs[i-1].reshape(-1))))\n",
    "#             print(f'OT shape: {ot.shape}')\n",
    "            \n",
    "            hs[i] = torch.mul(ot,torch.tanh(ct[i]))\n",
    "#             print(f'HS shape: {hs[i].shape}')\n",
    "            \n",
    "\n",
    "        return (list(hs.values()), hs[i])\n",
    "    \n",
    "        \n",
    "        # return a 2-tuple: (all hidden states, final hidden state)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_LSTM(torch.nn.Module):\n",
    "    def __init__(self,vocab_size,embedding_size, hidden_size, classes):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_size) \n",
    "        self.rnn_layer = LSTMLayer(embedding_size, hidden_size) \n",
    "        self.activation = torch.nn.Tanh()\n",
    "        self.classification = torch.nn.Linear(hidden_size, len(classes))\n",
    "         \n",
    "\n",
    "    def forward(self, x):  \n",
    "        x = self.activation(self.embedding(x))\n",
    "        hidden_states, x = self.rnn_layer(x) \n",
    "\n",
    "        x = self.activation(x)\n",
    "        x = self.classification(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_model = model_LSTM(len(vocab), embedding_size, hidden_size, classes)  \n",
    "cross_entropy = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(LSTM_model.parameters(), lr=0.005)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 0.010\n",
      "0.91\n",
      "[2,   100] loss: 0.011\n",
      "0.91\n",
      "[3,   100] loss: 0.011\n",
      "0.91\n",
      "complete training\n"
     ]
    }
   ],
   "source": [
    "# loop over the dataset multiple times\n",
    "for epoch in range(3):  \n",
    "\n",
    "    runloss = 0.0\n",
    "    for x, data in enumerate(train_vectorized):\n",
    "        # get the inputs; data is [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = LSTM_model(inputs)\n",
    "        loss = cross_entropy(outputs.view(1,-1), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        runloss = loss.item()\n",
    "        \n",
    "        #only 100 iterations\n",
    "        if x == 99:    \n",
    "            break\n",
    "    \n",
    "    #printing the loss\n",
    "    print(f'[{epoch + 1}, {x + 1:5d}] loss: {runloss/100:.3f}')\n",
    "    \n",
    "    #testing on test_data\n",
    "    acc = 0\n",
    "    for i, data in enumerate(test_vectorized):\n",
    "        test_tokens, label = data\n",
    "        predictions = Perceptron(test_tokens.view(1,-1))\n",
    "        predicted = torch.argmax(predictions.data,1)\n",
    "\n",
    "        if(predicted==label):\n",
    "            acc += 1\n",
    "        if(i==99):\n",
    "            break\n",
    "    print(acc/100)\n",
    "        \n",
    "print('complete training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
